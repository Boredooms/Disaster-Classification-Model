# AI Disaster Classification Model

## Overview

This project provides a web-based solution for classifying images of natural disasters. It utilizes a deep learning model built with TensorFlow and Keras to categorize images into four distinct classes: **Cyclone**, **Earthquake**, **Flood**, and **Wildfire**. The core of the project is a fine-tuned **ResNet50** model, which leverages transfer learning for high accuracy. The model is served through a Flask web application that allows users to upload an image and receive a prediction.

## Features

-   **Deep Learning Model**: A powerful Convolutional Neural Network (CNN) based on the ResNet50 architecture, fine-tuned for disaster classification.
-   **Transfer Learning**: Utilizes a model pre-trained on the ImageNet dataset to achieve high performance with a smaller, specialized dataset.
-   **Web Interface**: A user-friendly web application built with Flask for easy interaction and real-time predictions.
-   **Data Augmentation**: Artificially expands the training dataset by applying random transformations to images, improving the model's robustness and generalization.
-   **Git LFS**: Manages large files like the trained model (`.h5`) and image datasets, keeping the core repository lightweight.
-   **Structured Workflow**: Includes scripts for every stage of the machine learning lifecycle: data preparation, model training, evaluation, and prediction.

## Getting Started

### Prerequisites

-   Python 3.7+
-   pip
-   Git and Git LFS

### Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd Ai-Disaster-Classification-Model
    ```

2.  **Set up Git LFS:**
    ```bash
    git lfs install
    git lfs pull
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

The project is divided into several scripts to handle different parts of the workflow.

### 1. Data Preparation

Run the `prepare_dataset.py` script to clean and structure the raw image data.

```bash
python prepare_dataset.py
```
This will create a `dataset_structured` directory with `train`, `validation`, and `test` sets.

### 2. Model Training

Run the `train.py` script to train the model.

```bash
python train.py
```
The trained model will be saved as `saved_model/disaster_model.h5`.

### 3. Model Evaluation

Run the `evaluate.py` script to evaluate the model's performance on the test set.

```bash
python evaluate.py
```
This will print a classification report and save a confusion matrix image to `images/confusion_matrix.png`.

### 4. Prediction

You can make predictions using the web application or the command-line script.

#### Web Application

1.  **Run the Flask server:**
    ```bash
    python app.py
    ```
2.  **Access the application:**
    Open your web browser and navigate to `http://127.0.0.1:5000`.

#### Command-Line

Run the `predict.py` script with the path to an image.

```bash
python predict.py --image /path/to/your/image.jpg
```

## Project Structure

```
Ai-Disaster-Classification-Model/
├── .git/
├── .gitattributes
├── .gitignore
├── Cyclone_Wildfire_Flood_Earthquake_Dataset/  # Raw, unsorted image data.
├── dataset_structured/                         # Cleaned and organized train/val/test data.
├── images/
│   └── confusion_matrix.png                    # Saved confusion matrix plot.
├── saved_model/
│   └── disaster_model.h5                       # The trained and saved Keras model file.
├── static/
│   └── styles.css                              # CSS for the web interface.
├── templates/
│   └── index.html                              # HTML template for the web application.
├── app.py                                      # Flask backend for the web application.
├── evaluate.py                                 # Script to evaluate the model's performance.
├── model.py                                    # Defines the ResNet50 model architecture.
├── predict.py                                  # Command-line script for making predictions.
├── prepare_dataset.py                          # Script to clean and structure the dataset.
├── readme.md                                   # This file.
└── requirements.txt                            # Project dependencies.
```

## Files Description

*   **Cyclone_Wildfire_Flood_Earthquake_Dataset/**: This directory contains the raw, unsorted image data for the four disaster categories.
*   **dataset_structured/**: This directory is created by the `prepare_dataset.py` script and contains the cleaned and organized train, validation, and test data.
*   **images/**: This directory contains the confusion matrix plot generated by the `evaluate.py` script.
*   **saved_model/**: This directory contains the trained and saved Keras model file (`disaster_model.h5`).
*   **static/**: This directory contains the CSS file for the web interface.
*   **templates/**: This directory contains the HTML template for the web application.
*   **app.py**: This is the Flask backend for the web application. It handles the routing, model loading, image handling, and prediction.
*   **evaluate.py**: This script evaluates the model's performance on the test set. It prints a classification report and saves a confusion matrix image to the `images/` directory.
*   **model.py**: This script defines the ResNet50 model architecture.
*   **predict.py**: This is a command-line script for making predictions on a single image.
*   **prepare_dataset.py**: This script cleans and structures the raw image data into training, validation, and testing sets.
-   **readme.md**: This file.
*   **requirements.txt**: This file lists the project dependencies.

## Web Application

The project includes a simple web application to interact with the classification model.

### Frontend (`templates/index.html`)

-   **Framework**: Standard HTML5 with CSS for styling.
-   **Functionality**:
    -   Provides a clean and simple user interface.
    -   Features a file upload form where users can select an image (`.jpg`, `.jpeg`, `.png`).
    -   Displays the uploaded image and the model's prediction result.

### Backend (`app.py`)

-   **Framework**: **Flask**.
-   **Functionality**:
    -   **Routing**: Defines the endpoints for the web application.
    -   **Model Loading**: Loads the pre-trained `disaster_model.h5`.
    -   **Image Handling**: Receives and preprocesses the uploaded image.
    -   **Prediction**: Passes the image to the model to get a prediction.
    -   **Renders Results**: Renders the `index.html` template with the prediction result.

## Python Scripts Explained

### `model.py`

**Purpose**: To define the architecture of the Convolutional Neural Network (CNN) using transfer learning.

-   **Process**:
    1.  **Loads ResNet50**: It loads the `ResNet50` model pre-trained on ImageNet, excluding the final classification layer (`include_top=False`).
    2.  **Fine-Tuning**: It freezes the layers of the base model to retain the learned features from ImageNet.
    3.  **Custom Head**: A new classification head is added on top of the ResNet50 base. This head consists of:
        -   `GlobalAveragePooling2D`: To reduce the spatial dimensions.
        -   `Dense` layer with `relu` activation.
        -   A final `Dense` layer with a `softmax` activation function to output the probabilities for each disaster class.
    4.  **Compilation**: The model is compiled with the `Adam` optimizer, `categorical_crossentropy` loss function, and `accuracy` as the evaluation metric.

### `prepare_dataset.py`

**Purpose**: To clean the raw image data and structure it into training, validation, and testing sets.

-   **Process**:
    1.  **`clean_dataset`**: Iterates through all files and removes corrupt images.
    2.  **`split_dataset`**: Partitions the cleaned images into `train`, `validation`, and `test` sets and copies them into the `dataset_structured` directory.

### `train.py`

**Purpose**: To train the model on the prepared dataset.

-   **Process**:
    1.  **Data Augmentation**: It uses `ImageDataGenerator` to apply random transformations (rotation, zoom, flips, etc.) to the training images to prevent overfitting.
    2.  **Data Loading**: It creates data generators to load images in batches from the `train` and `validation` directories.
    3.  **Training Loop**: It calls `model.fit()` to train the model, monitoring performance on the validation set.
    4.  **Save Model**: After training, the final model is saved to `saved_model/disaster_model.h5`.

### `evaluate.py`

**Purpose**: To evaluate the performance of the trained model on the unseen test set.

-   **Process**:
    1.  **Load Model**: Loads the saved `disaster_model.h5`.
    2.  **Load Test Data**: Creates a data generator for the test set.
    3.  **Predict**: The model predicts the classes for all images in the test set.
    4.  **Generate Reports**:
        -   **Classification Report**: Generates a report with precision, recall, and F1-score for each class.
        -   **Confusion Matrix**: Creates and saves a confusion matrix plot.

### `predict.py`

**Purpose**: To provide a command-line interface for making a prediction on a single image.

-   **Process**:
    1.  **Argument Parsing**: Takes the image path as a command-line argument.
    2.  **Load Model**: Loads the saved `disaster_model.h5`.
    3.  **Preprocess Image**: Loads, resizes, and preprocesses the input image.
    4.  **Inference**: The preprocessed image is fed to the model for prediction.
    5.  **Output Result**: The script prints the predicted class and the confidence score.

## Detailed Workflow

The end-to-end workflow of the project is as follows:

1.  **Data Collection and Preparation**:
    -   The process begins with a raw dataset of disaster images located in the `Cyclone_Wildfire_Flood_Earthquake_Dataset` directory.
    -   The `prepare_dataset.py` script is executed to first clean this dataset by identifying and removing any corrupt or invalid image files.
    -   After cleaning, the script splits the dataset into three subsets: training (70%), validation (15%), and testing (15%).
    -   These subsets are then organized into a new directory, `dataset_structured`, which has a clear structure with `train`, `validation`, and `test` folders, each containing subfolders for the respective disaster categories.

2.  **Model Architecture and Definition**:
    -   The neural network architecture is defined in `model.py`.
    -   It leverages the **ResNet50** model, pre-trained on the massive ImageNet dataset, as a feature extractor. This is a core principle of **transfer learning**.
    -   The top classification layer of ResNet50 is removed, and a new custom head is added. This head is composed of a `GlobalAveragePooling2D` layer to reduce parameters, followed by a `Dense` layer with `relu` activation, and finally, a `Dense` output layer with `softmax` activation to produce the final class probabilities.
    -   The model is then compiled with the `Adam` optimizer, `categorical_crossentropy` as the loss function, and is set to monitor `accuracy`.

3.  **Model Training**:
    -   The `train.py` script orchestrates the training process.
    -   It uses `ImageDataGenerator` from Keras to perform **data augmentation** on the training set. This involves applying random transformations like rotations, zooms, and flips to the images in real-time, which helps the model generalize better and reduces overfitting.
    -   The script creates data generators that feed the training and validation data to the model in batches.
    -   The `model.fit()` method is called to begin training. The model learns from the training data, and its performance is checked against the validation data at the end of each epoch.
    -   Once training is complete, the final trained model, including its architecture and learned weights, is saved to `saved_model/disaster_model.h5`.

4.  **Model Evaluation**:
    -   To assess the model's performance on unseen data, the `evaluate.py` script is used.
    -   It loads the saved model and uses the `test` set, which the model has never been exposed to during training.
    -   The script generates two key evaluation metrics:
        -   A **Classification Report**, which provides detailed metrics like precision, recall, and F1-score for each disaster class.
        -   A **Confusion Matrix**, which is saved as an image and visually represents the model's performance, showing where it gets predictions right and where it makes mistakes.

5.  **Inference and Deployment**:
    -   Once the model is trained and evaluated, it is ready for making predictions. This can be done in two ways:
    -   **Web Application**: By running `app.py`, a Flask web server is started. Users can navigate to the provided URL in their browser, upload an image of a disaster, and the web application will display the model's prediction.
    -   **Command-Line Interface**: For quick, single-image predictions, the `predict.py` script can be used. It takes the path to an image as an argument and prints the predicted disaster class and the model's confidence score directly in the terminal.

## Conclusion

This project successfully demonstrates the development of a complete deep learning solution for a real-world problem. By leveraging transfer learning with the ResNet50 architecture, it achieves effective classification of natural disasters from images. The inclusion of a user-friendly web interface built with Flask makes the model accessible to non-technical users, while the command-line tools provide a straightforward way for developers to interact with the model.

**Potential applications** for this technology are vast, including aiding first responders in quickly assessing situations from satellite or drone imagery, helping news agencies to categorize and report on events, and contributing to automated environmental monitoring systems.

**Future improvements** could involve expanding the dataset to include more disaster categories, experimenting with other state-of-the-art model architectures like EfficientNet or Vision Transformers, and deploying the application to a cloud platform for global accessibility and scalability.